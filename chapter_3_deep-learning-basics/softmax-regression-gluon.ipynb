{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# softmax回归的简洁实现\n",
    "\n",
    "我们在[“线性回归的简洁实现”](linear-regression-gluon.ipynb)一节中已经了解了使用Gluon实现模型的便利。下面，让我们再次使用Gluon来实现一个softmax回归模型。首先导入所需的包或模块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "1"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import d2lzh as d2l\n",
    "from mxnet import gluon, init, autograd\n",
    "from mxnet.gluon import loss as gloss, nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取和读取数据\n",
    "\n",
    "我们仍然使用Fashion-MNIST数据集和上一节中设置的批量大小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "2"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义和初始化模型\n",
    "\n",
    "在[“softmax回归”](softmax-regression.ipynb)一节中提到，softmax回归的输出层是一个全连接层。因此，我们添加一个输出个数为10的全连接层。我们使用均值为0、标准差为0.01的正态分布随机初始化模型的权重参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    }
   },
   "outputs": [],
   "source": [
    "net = nn.Sequential()\n",
    "net.add(nn.Dense(1000))\n",
    "net.add(nn.Dense(1000))\n",
    "net.add(nn.Dense(10))\n",
    "net.initialize(init.Normal(sigma=0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## softmax和交叉熵损失函数\n",
    "\n",
    "如果做了上一节的练习，那么你可能意识到了分开定义softmax运算和交叉熵损失函数可能会造成数值不稳定。因此，Gluon提供了一个包括softmax运算和交叉熵损失计算的函数。它的数值稳定性更好。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "4"
    }
   },
   "outputs": [],
   "source": [
    "loss = gloss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义优化算法\n",
    "\n",
    "我们使用学习率为0.1的小批量随机梯度下降作为优化算法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "5"
    }
   },
   "outputs": [],
   "source": [
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练模型\n",
    "\n",
    "接下来，我们使用上一节中定义的训练函数来训练模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size,\n",
    "              params=None, lr=None, trainer=None):\n",
    "    start = time()\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n = 0.0, 0.0, 0\n",
    "        for X, y in train_iter:\n",
    "            with autograd.record():\n",
    "                y_hat = net(X)\n",
    "                l = loss(y_hat, y).sum()\n",
    "            l.backward()\n",
    "            if trainer is None:\n",
    "                d2l.sgd(params, lr, batch_size)\n",
    "            else:\n",
    "                trainer.step(batch_size)  # “softmax回归的简洁实现”一节将用到\n",
    "            y = y.astype('float32')\n",
    "            print(time()-start)\n",
    "            train_l_sum += l.asscalar()\n",
    "            print('----------',time()-start)\n",
    "            train_acc_sum += (y_hat.argmax(axis=1) == y).sum().asscalar()\n",
    "            n += y.size\n",
    "        test_acc = evaluate_accuracy(test_iter, net)\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f'\n",
    "              % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "6"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.004956483840942383\n",
      "---------- 0.016921520233154297\n",
      "0.019913911819458008\n",
      "---------- 0.027890443801879883\n",
      "0.030883073806762695\n",
      "---------- 0.040856361389160156\n",
      "0.04384803771972656\n",
      "---------- 0.05182671546936035\n",
      "0.054819345474243164\n",
      "---------- 0.06180095672607422\n",
      "0.06479263305664062\n",
      "---------- 0.07277131080627441\n",
      "0.07576322555541992\n",
      "---------- 0.08274435997009277\n",
      "0.08573698997497559\n",
      "---------- 0.09371495246887207\n",
      "0.09670734405517578\n",
      "---------- 0.10368800163269043\n",
      "0.10668110847473145\n",
      "---------- 0.11366105079650879\n",
      "0.1166536808013916\n",
      "---------- 0.12662649154663086\n",
      "0.12961983680725098\n",
      "---------- 0.13759732246398926\n",
      "0.13959240913391113\n",
      "---------- 0.14657330513000488\n",
      "0.1495664119720459\n",
      "---------- 0.15754413604736328\n",
      "0.15953898429870605\n",
      "---------- 0.16751718521118164\n",
      "0.17052173614501953\n",
      "---------- 0.1774909496307373\n",
      "0.18148159980773926\n",
      "---------- 0.19045686721801758\n",
      "0.19245147705078125\n",
      "---------- 0.20043063163757324\n",
      "0.20342230796813965\n",
      "---------- 0.21140050888061523\n",
      "0.2133946418762207\n",
      "---------- 0.2213730812072754\n",
      "0.2243657112121582\n",
      "---------- 0.23135066032409668\n",
      "0.2343430519104004\n",
      "---------- 0.24132657051086426\n",
      "0.24431657791137695\n",
      "---------- 0.2512974739074707\n",
      "0.2542896270751953\n",
      "---------- 0.26127076148986816\n",
      "0.26326608657836914\n",
      "---------- 0.2712438106536865\n",
      "0.2732386589050293\n",
      "---------- 0.2812185287475586\n",
      "0.28321218490600586\n",
      "---------- 0.29119062423706055\n",
      "0.2931859493255615\n",
      "---------- 0.3001668453216553\n",
      "0.3031594753265381\n",
      "---------- 0.31013989448547363\n",
      "0.3121352195739746\n",
      "---------- 0.3201136589050293\n",
      "0.3231065273284912\n",
      "---------- 0.3310844898223877\n",
      "0.33307838439941406\n",
      "---------- 0.34105730056762695\n",
      "0.34404969215393066\n",
      "---------- 0.3510315418243408\n",
      "0.3530256748199463\n",
      "---------- 0.3620014190673828\n",
      "0.3639969825744629\n",
      "---------- 0.3719747066497803\n",
      "0.3749673366546631\n",
      "---------- 0.3839435577392578\n",
      "0.38693737983703613\n",
      "---------- 0.39391613006591797\n",
      "0.3969099521636963\n",
      "---------- 0.40488648414611816\n",
      "0.40688204765319824\n",
      "---------- 0.4148600101470947\n",
      "0.41785430908203125\n",
      "---------- 0.4248332977294922\n",
      "0.42682909965515137\n",
      "---------- 0.4338092803955078\n",
      "0.4368016719818115\n",
      "---------- 0.4447803497314453\n",
      "0.4477722644805908\n",
      "---------- 0.4557507038116455\n",
      "0.4577453136444092\n",
      "---------- 0.46472692489624023\n",
      "0.46772003173828125\n",
      "---------- 0.4746997356414795\n",
      "0.47669506072998047\n",
      "---------- 0.4836761951446533\n",
      "0.485670804977417\n",
      "---------- 0.4946475028991699\n",
      "0.4966413974761963\n",
      "---------- 0.5046200752258301\n",
      "0.5066146850585938\n",
      "---------- 0.5145928859710693\n",
      "0.5165886878967285\n",
      "---------- 0.5265610218048096\n",
      "0.5295543670654297\n",
      "---------- 0.5374910831451416\n",
      "0.5404825210571289\n",
      "---------- 0.5474631786346436\n",
      "0.5514552593231201\n",
      "---------- 0.5604290962219238\n",
      "0.5634219646453857\n",
      "---------- 0.5724055767059326\n",
      "0.5743930339813232\n",
      "---------- 0.5843641757965088\n",
      "0.5863595008850098\n",
      "---------- 0.59433913230896\n",
      "0.5963339805603027\n",
      "---------- 0.6073040962219238\n",
      "0.6092982292175293\n",
      "---------- 0.6182754039764404\n",
      "0.6212666034698486\n",
      "---------- 0.629244327545166\n",
      "0.6327438354492188\n",
      "---------- 0.6427156925201416\n",
      "0.6447105407714844\n",
      "---------- 0.6526885032653809\n",
      "0.6556823253631592\n",
      "---------- 0.663658618927002\n",
      "0.6666510105133057\n",
      "---------- 0.6756272315979004\n",
      "0.6786203384399414\n",
      "---------- 0.6875960826873779\n",
      "0.6895906925201416\n",
      "---------- 0.6975681781768799\n",
      "0.7015581130981445\n",
      "---------- 0.7105405330657959\n",
      "0.7125282287597656\n",
      "---------- 0.7215046882629395\n",
      "0.724496603012085\n",
      "---------- 0.7314779758453369\n",
      "0.7344727516174316\n",
      "---------- 0.7434453964233398\n",
      "0.7464389801025391\n",
      "---------- 0.7554135322570801\n",
      "0.7584066390991211\n",
      "---------- 0.7673821449279785\n",
      "0.7703750133514404\n",
      "---------- 0.7793498039245605\n",
      "0.7813446521759033\n",
      "---------- 0.790320873260498\n",
      "0.7933127880096436\n",
      "---------- 0.8002943992614746\n",
      "0.8032865524291992\n",
      "---------- 0.8122615814208984\n",
      "0.8152542114257812\n",
      "---------- 0.8262243270874023\n",
      "0.829216718673706\n",
      "---------- 0.8381929397583008\n",
      "0.8411862850189209\n",
      "---------- 0.8481652736663818\n",
      "0.8511607646942139\n",
      "---------- 0.8601334095001221\n",
      "0.8621282577514648\n",
      "---------- 0.8721020221710205\n",
      "0.8740963935852051\n",
      "---------- 0.8820748329162598\n",
      "0.885068416595459\n",
      "---------- 0.8930461406707764\n",
      "0.8960380554199219\n",
      "---------- 0.9040162563323975\n",
      "0.907008171081543\n",
      "---------- 0.9149875640869141\n",
      "0.9179801940917969\n",
      "---------- 0.925957441329956\n",
      "0.9279530048370361\n",
      "---------- 0.9369280338287354\n",
      "0.9399206638336182\n",
      "---------- 0.9489052295684814\n",
      "0.9518899917602539\n",
      "---------- 0.9598672389984131\n",
      "0.9628591537475586\n",
      "---------- 0.9708371162414551\n",
      "0.973829984664917\n",
      "---------- 0.9828059673309326\n",
      "0.9857985973358154\n",
      "---------- 0.9937760829925537\n",
      "0.9967682361602783\n",
      "---------- 1.005744457244873\n",
      "1.0087370872497559\n",
      "---------- 1.016716480255127\n",
      "1.019707441329956\n",
      "---------- 1.0276861190795898\n",
      "1.0306785106658936\n",
      "---------- 1.0396535396575928\n",
      "1.0416491031646729\n",
      "---------- 1.0506243705749512\n",
      "1.052619457244873\n",
      "---------- 1.0615947246551514\n",
      "1.0635898113250732\n",
      "---------- 1.072566270828247\n",
      "1.0755584239959717\n",
      "---------- 1.0835363864898682\n",
      "1.0865285396575928\n",
      "---------- 1.0955047607421875\n",
      "1.0974993705749512\n",
      "---------- 1.1064863204956055\n",
      "1.1094682216644287\n",
      "---------- 1.11845064163208\n",
      "1.1224346160888672\n",
      "---------- 1.1334033012390137\n",
      "1.1353981494903564\n",
      "---------- 1.143376350402832\n",
      "1.1463711261749268\n",
      "---------- 1.1593341827392578\n",
      "1.162325143814087\n",
      "---------- 1.1722991466522217\n",
      "1.1752912998199463\n",
      "---------- 1.1842682361602783\n",
      "1.1862616539001465\n",
      "---------- 1.1952381134033203\n",
      "1.1972324848175049\n",
      "---------- 1.2062082290649414\n",
      "1.2082045078277588\n",
      "---------- 1.2171790599822998\n",
      "1.219175100326538\n",
      "---------- 1.227151870727539\n",
      "1.2291476726531982\n",
      "---------- 1.2391209602355957\n",
      "1.2421133518218994\n",
      "---------- 1.2510895729064941\n",
      "1.253082275390625\n",
      "---------- 1.2610657215118408\n",
      "1.2630562782287598\n",
      "---------- 1.2710356712341309\n",
      "1.274026870727539\n",
      "---------- 1.2810089588165283\n",
      "1.283003330230713\n",
      "---------- 1.291978359222412\n",
      "1.293973684310913\n",
      "---------- 1.3019516468048096\n",
      "1.3039472103118896\n",
      "---------- 1.312931776046753\n",
      "1.3149182796478271\n",
      "---------- 1.3238942623138428\n",
      "1.3268864154815674\n",
      "---------- 1.3348782062530518\n",
      "1.3378567695617676\n",
      "---------- 1.3448388576507568\n",
      "1.347830057144165\n",
      "---------- 1.3568050861358643\n",
      "1.3597981929779053\n",
      "---------- 1.3677761554718018\n",
      "1.3707685470581055\n",
      "---------- 1.379744052886963\n",
      "1.382737398147583\n",
      "---------- 1.3917131423950195\n",
      "1.393707036972046\n",
      "---------- 1.4026827812194824\n",
      "1.405674934387207\n",
      "---------- 1.4146509170532227\n",
      "1.4176435470581055\n",
      "---------- 1.4266183376312256\n",
      "1.4286141395568848\n",
      "---------- 1.4385862350463867\n",
      "1.441579818725586\n",
      "---------- 1.4505553245544434\n",
      "1.4535472393035889\n",
      "---------- 1.4615256786346436\n",
      "1.4645180702209473\n",
      "---------- 1.4724957942962646\n",
      "1.4754891395568848\n",
      "---------- 1.4844717979431152\n",
      "1.4864590167999268\n",
      "---------- 1.4964323043823242\n",
      "1.500425100326538\n",
      "---------- 1.509397268295288\n",
      "1.5123906135559082\n",
      "---------- 1.5213656425476074\n",
      "1.524357795715332\n",
      "---------- 1.5343315601348877\n",
      "1.5363261699676514\n",
      "---------- 1.5453011989593506\n",
      "1.5472965240478516\n",
      "---------- 1.5572688579559326\n",
      "1.5592641830444336\n",
      "---------- 1.5682401657104492\n",
      "1.5712339878082275\n",
      "---------- 1.5802085399627686\n",
      "1.5832021236419678\n",
      "---------- 1.5921757221221924\n",
      "1.5941705703735352\n",
      "---------- 1.603147029876709\n",
      "1.6061406135559082\n",
      "---------- 1.6161112785339355\n",
      "1.6191043853759766\n",
      "---------- 1.6270828247070312\n",
      "1.630075454711914\n",
      "---------- 1.638913631439209\n",
      "1.6418910026550293\n",
      "---------- 1.650866985321045\n",
      "1.6538584232330322\n",
      "---------- 1.6618363857269287\n",
      "1.664839744567871\n",
      "---------- 1.673804521560669\n",
      "1.6767969131469727\n",
      "---------- 1.6847751140594482\n",
      "1.687767744064331\n",
      "---------- 1.6977405548095703\n",
      "1.7007339000701904\n",
      "---------- 1.7087109088897705\n",
      "1.7107064723968506\n",
      "---------- 1.7196829319000244\n",
      "1.7226755619049072\n",
      "---------- 1.7306523323059082\n",
      "1.7336463928222656\n",
      "---------- 1.7416229248046875\n",
      "1.7436184883117676\n",
      "---------- 1.7515969276428223\n",
      "1.7545883655548096\n",
      "---------- 1.7625670433044434\n",
      "1.7655599117279053\n",
      "---------- 1.77353835105896\n",
      "1.7765328884124756\n",
      "---------- 1.7845191955566406\n",
      "1.7865042686462402\n",
      "---------- 1.7944815158843994\n",
      "1.7974750995635986\n",
      "---------- 1.805452823638916\n",
      "1.8084473609924316\n",
      "---------- 1.81742262840271\n",
      "1.819415807723999\n",
      "---------- 1.8283910751342773\n",
      "1.8303859233856201\n",
      "---------- 1.839362621307373\n",
      "1.842355489730835\n",
      "---------- 1.8513305187225342\n",
      "1.8533251285552979\n",
      "---------- 1.8613035678863525\n",
      "1.864295482635498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- 1.8732714653015137\n",
      "1.8762643337249756\n",
      "---------- 1.885239601135254\n",
      "1.8882322311401367\n",
      "---------- 1.899202585220337\n",
      "1.903193473815918\n",
      "---------- 1.9121668338775635\n",
      "1.9151611328125\n",
      "---------- 1.923137903213501\n",
      "1.926131248474121\n",
      "---------- 1.9351060390472412\n",
      "1.9371013641357422\n",
      "---------- 1.9450790882110596\n",
      "1.9480717182159424\n",
      "---------- 1.957047462463379\n",
      "1.9590415954589844\n",
      "---------- 1.9680185317993164\n",
      "1.9700126647949219\n",
      "---------- 1.978999137878418\n",
      "1.9809839725494385\n",
      "---------- 1.9899587631225586\n",
      "1.9929533004760742\n",
      "---------- 2.0009310245513916\n",
      "2.0029256343841553\n",
      "---------- 2.0119011402130127\n",
      "2.014892816543579\n",
      "---------- 2.022871255874634\n",
      "2.0258634090423584\n",
      "---------- 2.0328450202941895\n",
      "2.035837173461914\n",
      "---------- 2.044177770614624\n",
      "2.0461723804473877\n",
      "---------- 2.0541510581970215\n",
      "2.0581417083740234\n",
      "---------- 2.0661189556121826\n",
      "2.069112539291382\n",
      "---------- 2.0760927200317383\n",
      "2.079085350036621\n",
      "---------- 2.0880610942840576\n",
      "2.0920498371124268\n",
      "---------- 2.1000313758850098\n",
      "2.1030209064483643\n",
      "---------- 2.110001564025879\n",
      "2.112994432449341\n",
      "---------- 2.1219701766967773\n",
      "2.123964548110962\n",
      "---------- 2.1339385509490967\n",
      "2.135932445526123\n",
      "---------- 2.1434004306793213\n",
      "2.146393299102783\n",
      "---------- 2.154370069503784\n",
      "2.156364679336548\n",
      "---------- 2.1643433570861816\n",
      "2.1673367023468018\n",
      "---------- 2.1753146648406982\n",
      "2.177309036254883\n",
      "---------- 2.1862847805023193\n",
      "2.1882801055908203\n",
      "---------- 2.196258306503296\n",
      "2.1982531547546387\n",
      "---------- 2.206233263015747\n",
      "2.2092247009277344\n",
      "---------- 2.21720290184021\n",
      "2.220195770263672\n",
      "---------- 2.22717547416687\n",
      "2.2301695346832275\n",
      "---------- 2.2371490001678467\n",
      "2.2391443252563477\n",
      "---------- 2.248120069503784\n",
      "2.2511134147644043\n",
      "---------- 2.2590904235839844\n",
      "2.262082815170288\n",
      "---------- 2.271059513092041\n",
      "2.2730536460876465\n",
      "---------- 2.281033992767334\n",
      "2.284024715423584\n",
      "---------- 2.292001962661743\n",
      "2.293997287750244\n",
      "---------- 2.301975727081299\n",
      "2.3039703369140625\n",
      "---------- 2.311948537826538\n",
      "2.313943862915039\n",
      "---------- 2.32092547416687\n",
      "2.3239169120788574\n",
      "---------- 2.3318965435028076\n",
      "2.333890199661255\n",
      "---------- 2.3418688774108887\n",
      "2.3438644409179688\n",
      "---------- 2.351842164993286\n",
      "2.355834484100342\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-460ce82f425b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtime\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5000\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtrain_ch3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-44-706ee99e8871>\u001b[0m in \u001b[0;36mtrain_ch3\u001b[1;34m(net, train_iter, test_iter, loss, num_epochs, batch_size, params, lr, trainer)\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float32'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m             \u001b[0mtrain_l_sum\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masscalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'----------'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[0mtrain_acc_sum\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masscalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python36\\lib\\site-packages\\mxnet\\ndarray\\ndarray.py\u001b[0m in \u001b[0;36masscalar\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2012\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2013\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"The current array is not a scalar\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2014\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2015\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2016\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python36\\lib\\site-packages\\mxnet\\ndarray\\ndarray.py\u001b[0m in \u001b[0;36masnumpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1994\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1995\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_as\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1996\u001b[1;33m             ctypes.c_size_t(data.size)))\n\u001b[0m\u001b[0;32m   1997\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1998\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "num_epochs = 5000\n",
    "train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, None, None, trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 小结\n",
    "\n",
    "* Gluon提供的函数往往具有更好的数值稳定性。\n",
    "* 可以使用Gluon更简洁地实现softmax回归。\n",
    "\n",
    "## 练习\n",
    "\n",
    "* 尝试调一调超参数，如批量大小、迭代周期和学习率，看看结果会怎样。\n",
    "\n",
    "\n",
    "\n",
    "## 扫码直达[讨论区](https://discuss.gluon.ai/t/topic/740)\n",
    "\n",
    "![](../img/qr_softmax-regression-gluon.svg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
